<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>NG_ML02</title>
      <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/NG-ML02/"/>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/NG-ML02/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>吴恩达机器学习笔记--第一周</title>
      <link href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/NG-ML01/"/>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/NG-ML01/</url>
      
        <content type="html"><![CDATA[<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h1><h2 id="欢迎（Welcome）"><a href="#欢迎（Welcome）" class="headerlink" title="欢迎（Welcome）"></a>欢迎（Welcome）</h2><p>blahblah，机器学习很厉害，你们快来学</p><h2 id="什么是机器学习（What-is-Machine-Learning）"><a href="#什么是机器学习（What-is-Machine-Learning）" class="headerlink" title="什么是机器学习（What is Machine Learning）"></a>什么是机器学习（What is Machine Learning）</h2><p>Arthur Samuel“在进行特定编程的情况下，给予计算机 学习能力的领域”</p><p>Tom Mitchell“一个好的学习问题定义如下，一个程序被认为能从经验 E 中学习，解决任务 T，达到性能度量值 P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。我认为经验 E 就是程序上万次的自我练习的经验而任务 T 就是下棋。性能度量值 P 呢，就是它在与一 些新的对手比赛时，赢得比赛的概率”</p><h2 id="监督学习（Supervised-Learning）"><a href="#监督学习（Supervised-Learning）" class="headerlink" title="监督学习（Supervised  Learning）"></a>监督学习（Supervised  Learning）</h2><p>监督学习即给学习算法一个数据集。这个数据集由“正确答案”组成，再根据这些样本做出预测。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格， 即它们实际的售价然后运用学习算法，算出更多的正确答案</p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%871.png" alt="housingPricePre"></p><p>监督学习又叫回归问题（regression  problem）。我们通常试着推测一个连续值的结果，如房价。房价实际上是一系列离散的值，但我们将之看成实数、标量，所以又把它看成一个连续的值。回归即指：目标是预测一个连续的输出值 </p><p>分类问题（classification problem）：目标是预测离散值的输出，如肿瘤的良性/恶性<br><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%872.png" alt="brestCancer"></p><p>支持向量机（Support Vector Machine）算法：由于在机器学习问题中通常有大量的特征，故需要有算法能让电脑处理无穷多的特征<br><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%873.png" alt="classify"></p><h2 id="无监督学习（Unsupervised-Learning）、"><a href="#无监督学习（Unsupervised-Learning）、" class="headerlink" title="无监督学习（Unsupervised Learning）、"></a>无监督学习（Unsupervised Learning）、</h2><p>无监督学习的数据集中没有任何的标签、或有相同的标签，不知如何处理，也未告知每个数据点是什么。针对这样的数据集，无监督学习能判断出数据有n个不同的聚集簇。所以又叫做聚类算法（clustering algorithm）</p><p>可用于如基因分类、大数据中心（解决什么样的计算机易于协同工作）、社交网络分析（自动给出朋友分组，如常发邮件、Facebook好友etc）、市场分割（将顾客划分到不同的细分市场中，更有效的进行销售）、天文数据分析、鸡尾酒算法（分离两个音频）<br>鸡尾酒算法：[W,s,v] = svd((repmat(sum(x.<em>x,1),size(x,1),1).</em>x)*x’);</p><h1 id="2-单变量线性回归（Linear-Regression-with-One-Variable）"><a href="#2-单变量线性回归（Linear-Regression-with-One-Variable）" class="headerlink" title="2 单变量线性回归（Linear Regression with One Variable）"></a>2 单变量线性回归（Linear Regression with One Variable）</h1><h2 id="模型表示（Model-Representation）"><a href="#模型表示（Model-Representation）" class="headerlink" title="模型表示（Model Representation）"></a>模型表示（Model Representation）</h2><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%875.png" alt=""></p><p>我们将要用来描述这个回归问题的标记如下:<br>m    训练集中实例的数量    特征/输入变量<br>xy    目标变量/输出变量<br>(x,y)    训练集中的实例<br>(<script type="math/tex">x^{(i)}</script>, <script type="math/tex">y^{(i)}</script>) 第i个观察实例<br>h    学习算法的解决方案或函数也称为假设（hypothesis）</p><p>监督学习工作方式如下：<br><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%874.png" alt="工作方式"></p><p>h是一个x到y的映射，一种可能的表达：<script type="math/tex">h_{\Theta }=\Theta _{0} +\Theta _{1}x</script>，因为只含有一个特征/输入变量，所以叫做单变量线性回归问题</p><h2 id="代价函数（Cost-Function）"><a href="#代价函数（Cost-Function）" class="headerlink" title="代价函数（Cost Function）"></a>代价函数（Cost Function）</h2><p>需要选择合适的参数（parameters）和θ0、θ1，参数决定了得到的直线相对于训练集的准确程度。目标就在于选择出能使建模误差的平方和能够最小的模型参数，即代价函数 <script type="math/tex">J(\Theta _{0},\Theta _{1})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\Theta }(x^{(\Theta )})-y^{(\Theta )})^{2}</script><br>p.s.均方误差之所以除以2，是由于带了平方，之后要用到梯度下降法，需要求导，这样可把2消掉，一个简化计算的技巧</p><p>绘制一个等高线，三个坐标分别为θ0、θ1、J（θ0，θ1），即在三维空间中存在一个使J（θ0，θ1）最小的点</p><p>代价函数（平方误差函数、平方误差代价函数），之所以求误差平方和，因为这可能是解决回归问题最常用的手段</p><h2 id="代价函数的直观理解Ⅰ（Cost-Function-IntuitionⅠ）"><a href="#代价函数的直观理解Ⅰ（Cost-Function-IntuitionⅠ）" class="headerlink" title="代价函数的直观理解Ⅰ（Cost Function - IntuitionⅠ）"></a>代价函数的直观理解Ⅰ（Cost Function - IntuitionⅠ）</h2><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%877.png" alt=""></p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%878.png" alt=""></p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%879.png" alt=""></p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%8710.png" alt=""></p><h2 id="代价函数的直观理解Ⅱ（Cost-Function-IntuitionⅡ）"><a href="#代价函数的直观理解Ⅱ（Cost-Function-IntuitionⅡ）" class="headerlink" title="代价函数的直观理解Ⅱ（Cost Function - IntuitionⅡ）"></a>代价函数的直观理解Ⅱ（Cost Function - IntuitionⅡ）</h2><p>通过等高线图理解</p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%8711.png" alt=""></p><h2 id="梯度下降（Gradient-Descent）"><a href="#梯度下降（Gradient-Descent）" class="headerlink" title="梯度下降（Gradient Descent）"></a>梯度下降（Gradient Descent）</h2><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%8712.png" alt=""></p><p>基本思想：开始时随机选择一个参数的组合（θ0,θ1,…..,θn），计算代价函数，寻找下一个能让代价函数值下降最多的参数组合，持续这么做知道得到一个局部最小值（local minimum）。由于并未尝试所有的参数组合，故不能确定是否为全局最小值（global minimum）</p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%8713.png" alt=""></p><p>批量梯度下降（batch gradient descent）算法公式：</p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%8714.png" alt=""><br>α为学习速率（learning rate），决定了沿着能让代价函数下降程度最大方向迈出的步子有多大。在批量梯度下降中，每次都要同时让所有的参数减去学习速率乘以代价函数的导数（注意需要同时更新θ0,θ1）</p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%8714.png" alt=""></p><h2 id="梯度下降的直观理解（Gradient-Descent-Intuition）"><a href="#梯度下降的直观理解（Gradient-Descent-Intuition）" class="headerlink" title="梯度下降的直观理解（Gradient Descent Intuition）"></a>梯度下降的直观理解（Gradient Descent Intuition）</h2><p>梯度下降算法：<script type="math/tex">\Theta _{j}=\Theta _{j}-\alpha \frac{\partial }{\partial \Theta _{j}}J(\Theta )</script><br>描述：对θ赋值，使得J(θ)按梯度下降最快的方向进行，不断迭代直至得到局部最小值</p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%8716.png" alt=""></p><p>若α过小，即学习速率太小，则速度过慢，需要很多步才能达到全局最小值<br>若α过大，那么梯度下降算法可能会越过最低点，导致无法收敛，甚至发散<br>随着梯度下降算法的运行，移动的幅度会自动变得越来越小，直到最终移动幅度非常小，导数趋于0，则已经收敛到局部最小值。所有实际上没必要再另外减小α</p><h2 id="梯度下降的线性回归（Gradient-Descent-For-Linear-Regression）"><a href="#梯度下降的线性回归（Gradient-Descent-For-Linear-Regression）" class="headerlink" title="梯度下降的线性回归（Gradient Descent For Linear Regression）"></a>梯度下降的线性回归（Gradient Descent For Linear Regression）</h2><p>将梯度下降和代价函数结合，应用于具体的拟合直线的线性回归算法中，梯度下降算法和线性回归算法比较如下：</p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%8717.png" alt=""></p><p>对线性回归问题运用梯度下降算法，即求出代价函数的导数：</p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%8718.png" alt=""></p><p>此算法又被称为批量梯度下降，指的是，在梯度下降的每一步中，都用到了所有的训练样本</p><h1 id="3-线性代数回顾（Linear-Algebra-Review）"><a href="#3-线性代数回顾（Linear-Algebra-Review）" class="headerlink" title="3 线性代数回顾（Linear Algebra Review）"></a>3 线性代数回顾（Linear Algebra Review）</h1><p>矩阵和向量（Matrices and Vectors）<br>加法和标量乘法（Addition and Scalar Multiplication）<br>矩阵向量乘法（Matrix Vector Multiplication）</p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%8719.png" alt=""></p><p>矩阵乘法（Matrix-matrix Multiplication）</p><p><img src="http://pknivt4fw.bkt.clouddn.com/%E5%9B%BE%E7%89%8720.png" alt=""></p><p>奇异/退化矩阵（singular / degenerate matrix）：逆矩阵不存在的矩阵<br>逆矩阵、转置矩阵（Inverse and Transpose Matrix）</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>Hello World</title>
      <link href="/hello-world.html"/>
      <url>/hello-world.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="啊啊啊"><a href="#啊啊啊" class="headerlink" title="啊啊啊"></a>啊啊啊</h2><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo new "My New Post"</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>timeline</title>
      <link href="/timeline/index.html"/>
      <url>/timeline/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
